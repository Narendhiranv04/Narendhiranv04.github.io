<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Contract-validated HRL at IIIT Hyderabad · Narendhiran Vijayakumar</title>
    <meta
      name="description"
      content="Research journal detailing Narendhiran Vijayakumar's contract-driven hierarchical manipulation framework built at IIIT Hyderabad."
    />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=DM+Mono&family=DM+Sans:wght@400;500;600;700&family=Space+Grotesk:wght@400;600;700&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../assets/style.css" />
    <script defer src="../assets/script.js"></script>
  </head>
  <body class="research-page" data-theme="dark">
    <div class="research-page__inner">
      <header class="research-page__masthead">
        <a class="research-page__back" href="../index.html#research">← Back to research timeline</a>
        <button
          class="theme-toggle"
          type="button"
          data-js="theme-toggle"
          aria-label="Switch to light theme"
          title="Switch to light theme"
        >
          <span class="theme-toggle__icon" aria-hidden="true">☀️</span>
          <span class="theme-toggle__text">Light Mode</span>
        </button>
      </header>

      <article class="research-article">
        <header class="research-article__header">
          <div class="research-article__badge">
            <span>Hierarchical control · Recovery</span>
          </div>
          <p class="research-article__eyebrow">Task &amp; Motion Planning Intern · July 2025 – Present</p>
          <h1 class="research-article__title">Contracts that keep long-horizon tasks honest</h1>
          <p class="research-article__meta">IIIT Hyderabad · VLM planners with hierarchical RL recovery</p>
          <p class="research-article__lead">
            VLAs are great storytellers but terrible improvisers. I am building a contract-validated rescue layer so the robot can
            recover when the world tosses a new obstacle into a long-horizon plan.
          </p>
        </header>

        <div class="research-article__content">
          <p class="research-article__text">
            <strong>Motivation.</strong>
            I started noticing that most VLA systems look impressive only when the environment behaves perfectly. Everything’s
            static, clean, predictable. The moment an unknown object or obstacle appears, the entire sequence falls apart for most
            situations. That gap between ideal conditions and real ones felt like something worth working on. The question that
            shaped everything after was simple: <em>what should a robot do when something unexpected shows up halfway through a
            long-horizon task? (really really long ones)</em>
          </p>

          <p class="research-article__text">
            <strong>The problem I want to solve.</strong>
            VLAs alone can’t handle it. They understand tasks at a high level, but not what to <em>do</em> when plans fail
            mid-execution. Once an unseen obstacle appears, the problem shifts from perception to motion reasoning— essentially a
            Task-and-Motion Planning (TAMP) problem. Traditional TAMP handles this with <strong>PDDL</strong> libraries:
            pre-defined skills for every possible action. But it’s rigid and slow. Every time the task changes, you rewrite rules.
            That’s not scalable, and definitely not learnable.
          </p>

          <p class="research-article__text">
            <strong>My idea?</strong>
            I went for a <strong>two-system setup</strong>. The upper layer is a <strong>VLM task planner</strong> that breaks the
            goal into smaller “contracts.” Each contract has clear <strong>pre- and post-conditions</strong> defining what needs to
            be true before and after a step. Below that is a <strong>Hierarchical RL subsystem</strong> that handles what happens
            when something breaks. A <strong>validator</strong> monitors the contracts live; when a pre-condition fails, it sends that
            failure to a <strong>Mixture-of-Experts (MoE)</strong> policy. That high-level policy decides which learned primitive
            skill should fix the issue.
          </p>

          <p class="research-article__text">
            <strong>Training the recovery skills.</strong>
            The low-level primitives themselves aren’t scripted; they’re trained. I used <strong>Behavior Cloning (BC)</strong> to
            initialize them from demonstrations, then fine-tuned with <strong>PPO</strong> to make them adaptable. Each skill targets
            a specific failure mode i.e., clearing a blocked object, re-orienting a grasp, adjusting reach, etc. This way, instead
            of retrying the same plan blindly, the system can recover intelligently using learned behavior.
          </p>

          <p class="research-article__text">
            <strong>Implementation shenanigans.</strong>
            I built this on <strong>RLBench</strong>, using custom extended long-horizon scenes. Defining valid waypoint chains was
            easily the most frustrating part; half the initial paths were kinematically infeasible. I had to manually rewrite
            waypoint segmentation so the skill boundaries aligned with the contract definitions. Scene reconstruction and visual
            feature extraction were done using an <strong>R3M</strong> backbone, but balancing its dimensionality with rollout speed
            took a lot of iteration.
          </p>

          <p class="research-article__text">
            <strong>Getting it stable.</strong>
            Training was slow at first — PPO rollouts kept diverging until I added contract-based rewards. The validator itself
            needed constant tuning to detect when to actually call the HRL layer, instead of over-triggering. I also found out that
            even small differences in how the contracts were written could completely change performance.
          </p>

          <p class="research-article__text">
            <strong>Current stage.</strong>
            The lower-level skills are consistent now, and the validator is stable enough for most rollouts. The <strong>MoE
            high-level policy</strong> is still in progress; it decides which skill to trigger and when to stop using it. Integrating
            its feedback into the planner is tricky, but it’s improving.
          </p>

          <p class="research-article__text">
            <strong>Why I’m optimistic.</strong>
            Each run now generates logs of which contracts failed, what triggered the fallback, and which skill was selected. That
            traceability turned out to be one of the most useful parts — I can pinpoint exactly where the robot went wrong instead
            of staring at meaningless reward graphs.
          </p>

          <p class="research-article__closing">
            It’s not perfect yet, but it’s structured and explainable. The system doesn’t just fail; it tells me <em>how</em> it
            failed. Every broken pre-condition becomes training data. The end goal isn’t to make the robot flawless — it’s to make
            it self-aware enough to handle its own mistakes without me hand-holding it through every one.
          </p>
        </div>
      </article>

      <footer class="research-page__footer">
        <span>Last updated: July 2025</span>
        <a href="mailto:narendhiranv.nitt@gmail.com">Contact Narendhiran</a>
      </footer>
    </div>
  </body>
</html>
