<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Assistive torque prediction at Monash · Narendhiran Vijayakumar</title>
    <meta
      name="description"
      content="Detailed write-up of Narendhiran Vijayakumar's work on lightweight torque prediction and fuzzy supervision for sit-to-walk exoskeletons at Monash University."
    />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=DM+Mono&family=DM+Sans:wght@400;500;600;700&family=Space+Grotesk:wght@400;600;700&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../assets/style.css" />
    <script defer src="../assets/script.js"></script>
  </head>
  <body class="research-page" data-theme="dark">
    <div class="research-page__inner">
      <header class="research-page__masthead">
        <a class="research-page__back" href="../index.html#research">← Back to research timeline</a>
        <button
          class="theme-toggle"
          type="button"
          data-js="theme-toggle"
          aria-label="Switch to light theme"
          title="Switch to light theme"
        >
          <span class="theme-toggle__icon" aria-hidden="true">☀️</span>
          <span class="theme-toggle__text">Light Mode</span>
        </button>
      </header>

      <article class="research-article">
        <header class="research-article__header">
          <div class="research-article__badge">
            <span>Exoskeletons · Temporal Models</span>
          </div>
          <p class="research-article__eyebrow">Assistive Robotics Intern · Jan 2025 – May 2025</p>
          <h1 class="research-article__title">Fuzzy Logic–GRU Framework for Real-Time Sit-to-Walk Joint Torque Estimation</h1>
          <p class="research-article__meta">Monash University · Sit-to-walk exoskeleton controller</p>
          <p class="research-article__lead">
            I compressed a gated recurrent network to 68 k parameters and coupled it with a fuzzy supervisor, enabling our
            sit-to-walk exoskeleton to generate torque within 20 ms while maintaining accuracy under noisy IMU and EMG
            signals.
            <span style="color: #ff4d4f;">Project webpage coming soon. (with results, code, and demos)</span>
          </p>
        </header>

        <div class="research-article__content">
          <p class="research-article__text">
            <strong>Where I was.</strong>
            I was still exploring about robotics. I didn’t want to lock into one subfield of robotics too early, so I kept touching
            different lines of work just to understand what “helping a robot act in the world” actually means in practice. That’s how I
            ended up in assistive robotics — specifically lower-limb exoskeletons for elderly users. This wasn’t something I could’ve
            worked on at my own university, so when I got the chance, I jumped on it and focused on the software/control side.
          </p>

          <p class="research-article__text">
            <strong>What we were trying to solve.</strong>
            The setup sounds simple: detect when a person is about to initiate sit-to-walk gait motion, and then generate the assistive
            torques for the hip and knee joint over time so that the exoskeleton can help them move. Two parts — intent prediction and
            torque prediction in one pipeline, running live on the device. The “not so simple” part is that you only get a few
            milliseconds to decide, and being late is not acceptable. That’s precisely what I wanted to achieve i.e., low latency.
          </p>

          <p class="research-article__text">
            <strong>How I approached it.</strong>
            The first thing I noticed is that you can’t just throw a giant deep network at this. On hardware, every extra millisecond
            matters. I needed a balance: good enough accuracy, extremely low latency, fully deterministic behavior. So instead of
            starting with something heavy, I went in the opposite direction: intent detection using fuzzy logic. I studied fuzzy
            inference systems — they’re fast, interpretable, and great for temporal classification. I built a Mamdani-style fuzzy
            inference layer that basically answers one question in real time: “Is the movement you’re seeing an intentional gait
            initiation, or just noise / random shifting in the chair?”
          </p>

          <p class="research-article__text">
            <strong>Choosing the predictor.</strong>
            After intent detection, you still need to predict the actual joint torques over a short horizon. I tried different recurrent
            models: vanilla RNNs, LSTMs, GRUs. GRUs ended up being the sweet spot. They were lightweight enough to run quickly but still
            captured the temporal structure of hip/knee torque evolution without collapsing. I did push a bit further — I tried
            attention-style tweaks and alternative encoder–decoder variants to see if we could squeeze out more stability once deployed
            on the real exoskeleton. That helped close the last gap in performance when we moved from offline eval to on-device testing.
          </p>

          <p class="research-article__text">
            <strong>Making it run on the device.</strong>
            Getting the whole thing to actually run as one loop was its own headache. The fuzzy intent module and the GRU torque
            predictor had to live inside the same C++ runtime. That meant: converting learned weights cleanly, getting rid of
            Python/NumPy assumptions, and shaving latency at every step. Tiny details like how tensors were laid out in memory started to
            matter. But once that was nailed down, it became a single call: take the most recent motion data, confirm “yes, this is
            actually a sit-to-walk start,” and immediately output the assistive torque profile.
          </p>

          <p class="research-article__text">
            <strong>Where it landed.</strong>
            The final pipeline is fully C++ and runs fast enough for real-time use on the exoskeleton. No huge black-box model, no
            cloud, no delay. It predicts what help the user needs, right now, and supplies those torques to the hardware. It’s not trying
            to solve “general human motion understanding.” It’s focused on one very specific moment that actually matters for elderly
            users — getting up and starting to walk safely.
          </p>

          <p class="research-article__closing">
            It’s a small slice of assistive robotics, but it made the problem feel real to me. Learnt a lot about temporal models in this
            intern experience.
          </p>
        </div>
      </article>

      <footer class="research-page__footer">
        <span>Last updated: May 2025</span>
        <a href="mailto:narendhiranv.nitt@gmail.com">Contact Narendhiran</a>
      </footer>
    </div>
  </body>
</html>
